{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9eada719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !/usr/bin/env python3\n",
    "# a vibe coded web crawler by Claude\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import time\n",
    "import re\n",
    "from typing import List, Dict\n",
    "\n",
    "class JobCrawler:\n",
    "    def __init__(self, keywords: List[str], max_pages: int = 5):\n",
    "        \"\"\"\n",
    "        Initialize the job crawler.\n",
    "        \n",
    "        Args:\n",
    "            keywords: List of keywords to search for in job postings\n",
    "            max_pages: Maximum number of pages to crawl per URL\n",
    "        \"\"\"\n",
    "        self.keywords = [kw.lower() for kw in keywords]\n",
    "        self.max_pages = max_pages\n",
    "        self.results = []\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        }\n",
    "    \n",
    "    def fetch_page(self, url: str) -> str:\n",
    "        \"\"\"Fetch HTML content from a URL.\"\"\"\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            return response.text\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching {url}: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def extract_text(self, html: str) -> str:\n",
    "        \"\"\"Extract readable text from HTML.\"\"\"\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        # Remove script and style elements\n",
    "        for script in soup(['script', 'style', 'nav', 'footer', 'header']):\n",
    "            script.decompose()\n",
    "        \n",
    "        text = soup.get_text()\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        text = ' '.join(chunk for chunk in chunks if chunk)\n",
    "        \n",
    "        return text.lower()\n",
    "    \n",
    "    def contains_keywords(self, text: str) -> List[str]:\n",
    "        \"\"\"Check if text contains any of the keywords.\"\"\"\n",
    "        found = []\n",
    "        for keyword in self.keywords:\n",
    "            if keyword in text:\n",
    "                found.append(keyword)\n",
    "        return found\n",
    "    \n",
    "    def extract_job_links(self, html: str, base_url: str) -> List[str]:\n",
    "        \"\"\"Extract potential job posting links from a page.\"\"\"\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        links = set()\n",
    "        \n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            full_url = urljoin(base_url, href)\n",
    "            \n",
    "            # Filter for job-related URLs\n",
    "            job_indicators = ['job', 'position', 'career', 'opening', 'posting']\n",
    "            if any(indicator in full_url.lower() for indicator in job_indicators):\n",
    "                links.add(full_url)\n",
    "        \n",
    "        return list(links)[:self.max_pages]\n",
    "    \n",
    "    def crawl_url(self, url: str):\n",
    "        \"\"\"Crawl a single job board URL.\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Crawling: {url}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Fetch main page\n",
    "        html = self.fetch_page(url)\n",
    "        if not html:\n",
    "            return\n",
    "        \n",
    "        # Check main page for keywords\n",
    "        text = self.extract_text(html)\n",
    "        found_keywords = self.contains_keywords(text)\n",
    "        \n",
    "        if found_keywords:\n",
    "            self.results.append({\n",
    "                'url': url,\n",
    "                'keywords_found': found_keywords,\n",
    "                'title': self.extract_title(html)\n",
    "            })\n",
    "            print(f\"✓ Found keywords on main page: {', '.join(found_keywords)}\")\n",
    "        \n",
    "        # Extract and crawl job links\n",
    "        job_links = self.extract_job_links(html, url)\n",
    "        print(f\"Found {len(job_links)} potential job postings to check...\")\n",
    "        \n",
    "        for i, job_url in enumerate(job_links, 1):\n",
    "            print(f\"  Checking ({i}/{len(job_links)}): {job_url[:60]}...\")\n",
    "            \n",
    "            job_html = self.fetch_page(job_url)\n",
    "            if job_html:\n",
    "                job_text = self.extract_text(job_html)\n",
    "                found = self.contains_keywords(job_text)\n",
    "                \n",
    "                if found:\n",
    "                    self.results.append({\n",
    "                        'url': job_url,\n",
    "                        'keywords_found': found,\n",
    "                        'title': self.extract_title(job_html)\n",
    "                    })\n",
    "                    print(f\"    ✓ Match! Keywords: {', '.join(found)}\")\n",
    "            \n",
    "            time.sleep(1)  # Be polite, don't hammer the server\n",
    "    \n",
    "    def extract_title(self, html: str) -> str:\n",
    "        \"\"\"Extract page title from HTML.\"\"\"\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        title_tag = soup.find('title')\n",
    "        return title_tag.get_text().strip() if title_tag else \"No title\"\n",
    "    \n",
    "    def crawl(self, urls: List[str]):\n",
    "        \"\"\"Crawl multiple job board URLs.\"\"\"\n",
    "        print(f\"\\nStarting crawl for keywords: {', '.join(self.keywords)}\")\n",
    "        \n",
    "        for url in urls:\n",
    "            self.crawl_url(url)\n",
    "            time.sleep(2)  # Delay between different sites\n",
    "        \n",
    "        self.print_results()\n",
    "    \n",
    "    def print_results(self):\n",
    "        \"\"\"Print the crawl results.\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"CRAWL COMPLETE - Found {len(self.results)} matching pages\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        if not self.results:\n",
    "            print(\"No matches found for the specified keywords.\")\n",
    "            return\n",
    "        \n",
    "        for i, result in enumerate(self.results, 1):\n",
    "            print(f\"{i}. {result['title']}\")\n",
    "            print(f\"   URL: {result['url']}\")\n",
    "            print(f\"   Keywords: {', '.join(result['keywords_found'])}\")\n",
    "            print()\n",
    "    \n",
    "    def save_results(self, filename: str = \"job_results.txt\"):\n",
    "        \"\"\"Save results to a text file.\"\"\"\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"Job Crawler Results\\n\")\n",
    "            f.write(f\"Keywords: {', '.join(self.keywords)}\\n\")\n",
    "            f.write(f\"Total matches: {len(self.results)}\\n\")\n",
    "            f.write(\"=\"*60 + \"\\n\\n\")\n",
    "            \n",
    "            for i, result in enumerate(self.results, 1):\n",
    "                f.write(f\"{i}. {result['title']}\\n\")\n",
    "                f.write(f\"   URL: {result['url']}\\n\")\n",
    "                f.write(f\"   Keywords: {', '.join(result['keywords_found'])}\\n\\n\")\n",
    "        \n",
    "        print(f\"Results saved to {filename}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2a7f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create lists of job boards to crawl\n",
    "\n",
    "#Lists can be swapped out as needed\n",
    "\n",
    "top_scores_list = [\n",
    "    \"https://weworkremotely.com\",\n",
    "    \"https://www.usebraintrust.com\",\n",
    "    \"https://nodesk.co\",\n",
    "    \"https://remoteok.com\",\n",
    "    \"https://www.crossover.com/jobs\",\n",
    "    \"https://remotive.com\",\n",
    "    \"https://www.workingnomads.com\",\n",
    "    \"https://www.flexjobs.com\",\n",
    "    \"https://jobspresso.co\",\n",
    "    \"https://builtin.com\",\n",
    "    \"https://powertofly.com\",\n",
    "    \"https://remotewoman.com\"\n",
    "]\n",
    "\n",
    "#flexjobs gives errors unless logged in\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba89e0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create list of keywords to search for during crawl\n",
    "\n",
    "# Core Role Keywords\n",
    "role_keywords = [\n",
    "    \"program manager\", \"project manager\", \"product manager\",\n",
    "    \"technical program manager\", \"technical project manager\",\n",
    "    \"senior program manager\", \"senior product manager\",\n",
    "    \"delivery manager\", \"engineering program manager\",\n",
    "    \"r&d program manager\", \"implementation manager\",\n",
    "    \"pmo\", \"agile\", \"scrum\", \"kanban\"\n",
    "]\n",
    "\n",
    "# Tech Industry Keywords\n",
    "tech_keywords = [\n",
    "    \"software\", \"ai\", \"machine learning\", \"data science\", \"cloud\",\n",
    "    \"saas\", \"devops\", \"full stack\", \"backend\", \"frontend\",\n",
    "    \"cybersecurity\", \"infrastructure\", \"platform\", \"systems\",\n",
    "    \"digital transformation\", \"innovation\", \"startup\", \"venture\",\n",
    "    \"technology\", \"it\", \"analytics\", \"product development\"\n",
    "]\n",
    "\n",
    "# Remote / Hybrid Keywords\n",
    "remote_keywords = [\n",
    "    \"remote\", \"hybrid\", \"distributed\", \"work from anywhere\",\n",
    "    \"global\", \"flexible\", \"telecommute\", \"digital nomad\",\n",
    "    \"work from home\", \"async\", \"worldwide\"\n",
    "]\n",
    "\n",
    "# Compensation / Seniority Keywords\n",
    "salary_keywords = [\n",
    "    \"senior\", \"director\", \"lead\", \"principal\", \"head\", \"executive\",\n",
    "    \"$130k\", \"$150k\", \"six-figure\", \"high compensation\",\n",
    "    \"experienced\", \"strategy\", \"enterprise\", \"scale\", \"leadership\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2a0634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting crawl for keywords: program manager, project manager, product manager, technical program manager, technical project manager, senior program manager, senior product manager, delivery manager, engineering program manager, r&d program manager, implementation manager, pmo, agile, scrum, kanban\n",
      "\n",
      "============================================================\n",
      "Crawling: https://weworkremotely.com\n",
      "============================================================\n",
      "✓ Found keywords on main page: project manager, product manager, agile, scrum\n",
      "Found 10 potential job postings to check...\n",
      "  Checking (1/10): https://weworkremotely.com/remote-jobs/asc-engineered-soluti...\n",
      "Error fetching https://weworkremotely.com/remote-jobs/asc-engineered-solutions-crm-administrator: 403 Client Error: Forbidden for url: https://weworkremotely.com/remote-jobs/asc-engineered-solutions-crm-administrator\n",
      "  Checking (2/10): https://weworkremotely.com/remote-jobs/whitebridge-ltd-junio...\n",
      "Error fetching https://weworkremotely.com/remote-jobs/whitebridge-ltd-junior-crypto-analyst-trader-remote-training-included: 403 Client Error: Forbidden for url: https://weworkremotely.com/remote-jobs/whitebridge-ltd-junior-crypto-analyst-trader-remote-training-included\n",
      "  Checking (3/10): https://weworkremotely.com/remote-jobs/maxval-ip-services-te...\n",
      "Error fetching https://weworkremotely.com/remote-jobs/maxval-ip-services-technical-lead-devops: 403 Client Error: Forbidden for url: https://weworkremotely.com/remote-jobs/maxval-ip-services-technical-lead-devops\n",
      "  Checking (4/10): https://weworkremotely.com/remote-jobs/toptal-android-develo...\n",
      "Error fetching https://weworkremotely.com/remote-jobs/toptal-android-developer-1: 403 Client Error: Forbidden for url: https://weworkremotely.com/remote-jobs/toptal-android-developer-1\n",
      "  Checking (5/10): https://weworkremotely.com/job-seekers/account/register?aler...\n",
      "Error fetching https://weworkremotely.com/job-seekers/account/register?alert=To+view+full+job+details%2C+please+sign+in+and+subscribe+to+one+of+our+plans.: 403 Client Error: Forbidden for url: https://weworkremotely.com/job-seekers/account/register?alert=To+view+full+job+details%2C+please+sign+in+and+subscribe+to+one+of+our+plans.\n",
      "  Checking (6/10): https://weworkremotely.com/categories/remote-back-end-progra...\n",
      "  Checking (7/10): https://weworkremotely.com/remote-jobs/proxify-ab-senior-dev...\n",
      "Error fetching https://weworkremotely.com/remote-jobs/proxify-ab-senior-devops-engineer-aws-3: 403 Client Error: Forbidden for url: https://weworkremotely.com/remote-jobs/proxify-ab-senior-devops-engineer-aws-3\n",
      "  Checking (8/10): https://weworkremotely.com/remote-jobs/nogigiddy-remote-cust...\n",
      "Error fetching https://weworkremotely.com/remote-jobs/nogigiddy-remote-customer-service-rep-up-to-19hour-no-degree-needed-1: 403 Client Error: Forbidden for url: https://weworkremotely.com/remote-jobs/nogigiddy-remote-customer-service-rep-up-to-19hour-no-degree-needed-1\n",
      "  Checking (9/10): https://weworkremotely.com/remote-jobs/wisepath-group-financ...\n",
      "Error fetching https://weworkremotely.com/remote-jobs/wisepath-group-financial-advisor: 403 Client Error: Forbidden for url: https://weworkremotely.com/remote-jobs/wisepath-group-financial-advisor\n",
      "  Checking (10/10): https://weworkremotely.com/remote-full-time-jobs#job-listing...\n",
      "\n",
      "============================================================\n",
      "Crawling: https://www.usebraintrust.com\n",
      "============================================================\n",
      "Found 1 potential job postings to check...\n",
      "  Checking (1/1): https://app.usebraintrust.com/jobs/...\n",
      "\n",
      "============================================================\n",
      "Crawling: https://nodesk.co\n",
      "============================================================\n",
      "Found 10 potential job postings to check...\n",
      "  Checking (1/10): https://nodesk.co/remote-jobs/sales/...\n",
      "  Checking (2/10): https://nodesk.co/remote-jobs/aha-product-success-specialist...\n",
      "    ✓ Match! Keywords: product manager, senior product manager\n",
      "  Checking (3/10): https://nodesk.co/remote-jobs/part-time/...\n",
      "    ✓ Match! Keywords: project manager\n",
      "  Checking (4/10): https://nodesk.co/remote-jobs/remote-first/...\n",
      "    ✓ Match! Keywords: implementation manager\n",
      "  Checking (5/10): https://nodesk.co/remote-jobs/buzzybooth-customer-success-ma...\n",
      "  Checking (6/10): https://nodesk.co/remote-jobs/entry-level/...\n",
      "    ✓ Match! Keywords: product manager\n",
      "  Checking (7/10): https://nodesk.co/remote-jobs/product/...\n",
      "    ✓ Match! Keywords: product manager, senior product manager\n",
      "  Checking (8/10): https://nodesk.co/remote-jobs/reddit-senior-software-enginee...\n",
      "  Checking (9/10): https://nodesk.co/remote-jobs/new/...\n",
      "  Checking (10/10): https://nodesk.co/remote-jobs/us/...\n",
      "    ✓ Match! Keywords: program manager, project manager, product manager, senior program manager, senior product manager\n",
      "\n",
      "============================================================\n",
      "Crawling: https://remoteok.com\n",
      "============================================================\n",
      "Found 10 potential job postings to check...\n",
      "  Checking (1/10): https://remoteok.com/remote-software-jobs...\n",
      "  Checking (2/10): https://www.producthunt.com/posts/remote-ok-jobs-api...\n",
      "Error fetching https://www.producthunt.com/posts/remote-ok-jobs-api: 403 Client Error: Forbidden for url: https://www.producthunt.com/posts/remote-ok-jobs-api\n",
      "  Checking (3/10): https://web3.career/?ref=remoteok...\n",
      "    ✓ Match! Keywords: project manager, product manager\n",
      "  Checking (4/10): https://remoteok.com/remote-support-jobs...\n",
      "  Checking (5/10): https://remoteok.com/remote-lead-jobs...\n",
      "  Checking (6/10): https://remoteok.com/remote-jobs...\n",
      "  Checking (7/10): https://remoteok.com/remote-senior-jobs...\n",
      "  Checking (8/10): https://remoteok.com/remote-engineer-jobs...\n",
      "  Checking (9/10): https://remoteok.com/remote-management-jobs...\n",
      "  Checking (10/10): https://remoteok.com/remote-growth-jobs...\n",
      "\n",
      "============================================================\n",
      "Crawling: https://www.crossover.com/jobs\n",
      "============================================================\n",
      "Found 0 potential job postings to check...\n",
      "\n",
      "============================================================\n",
      "Crawling: https://remotive.com/\n",
      "============================================================\n",
      "Found 10 potential job postings to check...\n",
      "  Checking (1/10): https://remotive.com/remote-jobs/devops/senior-devops-engine...\n",
      "  Checking (2/10): https://remotive.com/remote-jobs/api...\n",
      "  Checking (3/10): https://remotive.com/job/post/new...\n",
      "  Checking (4/10): https://remotive.com/remote-jobs/hr...\n",
      "  Checking (5/10): https://remotive.com/remote-jobs/marketing...\n",
      "    ✓ Match! Keywords: program manager\n",
      "  Checking (6/10): https://remotive.com/remote-jobs/customer-support/client-sup...\n",
      "  Checking (7/10): https://t.me/remotejobsenglish/...\n",
      "  Checking (8/10): https://remotive.com/remote-jobs/marketing/office-assistant-...\n",
      "  Checking (9/10): https://remotive.com/remote-jobs/writing/freelance-writer-11...\n",
      "  Checking (10/10): https://remotive.com/remote-jobs/software-dev/ios-developer-...\n",
      "    ✓ Match! Keywords: agile\n",
      "\n",
      "============================================================\n",
      "Crawling: https://www.workingnomads.com/\n",
      "============================================================\n",
      "Found 10 potential job postings to check...\n",
      "  Checking (1/10): https://www.workingnomads.com/remote-philippines-jobs...\n",
      "  Checking (2/10): https://www.workingnomads.com/remote-usa-jobs...\n",
      "  Checking (3/10): https://www.workingnomads.com/remote-writing-jobs...\n",
      "  Checking (4/10): https://www.workingnomads.com/remote-finance-jobs...\n",
      "  Checking (5/10): https://www.workingnomads.com/remote-france-jobs...\n",
      "  Checking (6/10): https://www.workingnomads.com/remote-india-jobs...\n",
      "  Checking (7/10): https://www.workingnomads.com/remote-japan-jobs...\n",
      "  Checking (8/10): https://www.workingnomads.com/remote-marketing-jobs...\n",
      "  Checking (9/10): https://www.workingnomads.com/remote-cyber-security-jobs...\n",
      "  Checking (10/10): https://www.workingnomads.com/remote-australia-jobs...\n",
      "\n",
      "============================================================\n",
      "Crawling: https://jobspresso.co/\n",
      "============================================================\n",
      "Found 10 potential job postings to check...\n",
      "  Checking (1/10): https://jobspresso.co/remote-non-tech-jobs/...\n",
      "  Checking (2/10): https://jobspresso.co/job/manager-technical-support-level-1/...\n",
      "  Checking (3/10): https://jobspresso.co/remote-ai-data-jobs/...\n",
      "  Checking (4/10): https://jobspresso.co/job/ai-engineer-12-months-18-months-fi...\n",
      "    ✓ Match! Keywords: product manager\n",
      "  Checking (5/10): https://jobspresso.co/contact/...\n",
      "  Checking (6/10): https://jobspresso.co/job/software-engineer-security-acceler...\n",
      "  Checking (7/10): https://twitter.com/Jobspresso...\n",
      "  Checking (8/10): https://www.techrepublic.com/article/how-to-land-a-remote-jo...\n",
      "Error fetching https://www.techrepublic.com/article/how-to-land-a-remote-job-and-thrive-in-it/: 403 Client Error: Forbidden for url: https://www.techrepublic.com/article/how-to-land-a-remote-job-and-thrive-in-it/\n",
      "  Checking (9/10): https://jobspresso.co/job/atm-investors-worldwide-various-op...\n",
      "  Checking (10/10): https://jobspresso.co/remote-sales-jobs/...\n",
      "\n",
      "============================================================\n",
      "Crawling: https://www.flexjobs.com\n",
      "============================================================\n",
      "Error fetching https://www.flexjobs.com: HTTPSConnectionPool(host='www.flexjobs.com', port=443): Read timed out. (read timeout=10)\n",
      "\n",
      "============================================================\n",
      "CRAWL COMPLETE - Found 11 matching pages\n",
      "============================================================\n",
      "\n",
      "1. We Work Remotely: Advanced Remote Job Search\n",
      "   URL: https://weworkremotely.com\n",
      "   Keywords: project manager, product manager, agile, scrum\n",
      "\n",
      "2. Remote Product Success Specialist at Aha! - NoDesk\n",
      "   URL: https://nodesk.co/remote-jobs/aha-product-success-specialist/\n",
      "   Keywords: product manager, senior product manager\n",
      "\n",
      "3. Remote Part-Time Jobs - NoDesk\n",
      "   URL: https://nodesk.co/remote-jobs/part-time/\n",
      "   Keywords: project manager\n",
      "\n",
      "4. Remote-First Jobs - NoDesk\n",
      "   URL: https://nodesk.co/remote-jobs/remote-first/\n",
      "   Keywords: implementation manager\n",
      "\n",
      "5. Remote Entry-Level Jobs - NoDesk\n",
      "   URL: https://nodesk.co/remote-jobs/entry-level/\n",
      "   Keywords: product manager\n",
      "\n",
      "6. Remote Product Jobs - NoDesk\n",
      "   URL: https://nodesk.co/remote-jobs/product/\n",
      "   Keywords: product manager, senior product manager\n",
      "\n",
      "7. Remote Jobs in the US - NoDesk\n",
      "   URL: https://nodesk.co/remote-jobs/us/\n",
      "   Keywords: program manager, project manager, product manager, senior program manager, senior product manager\n",
      "\n",
      "8. Web3 Jobs: Blockchain, Smart Contract and Crypto Jobs\n",
      "   URL: https://web3.career/?ref=remoteok\n",
      "   Keywords: project manager, product manager\n",
      "\n",
      "9. Remote Marketing Jobs | Remotive.com\n",
      "   URL: https://remotive.com/remote-jobs/marketing\n",
      "   Keywords: program manager\n",
      "\n",
      "10. [Hiring] iOS Developer @nooro\n",
      "   URL: https://remotive.com/remote-jobs/software-dev/ios-developer-1956455\n",
      "   Keywords: agile\n",
      "\n",
      "11. AI Engineer (12 months or 18 months fixed-term) | Jobspresso\n",
      "   URL: https://jobspresso.co/job/ai-engineer-12-months-18-months-fixed-term/\n",
      "   Keywords: product manager\n",
      "\n",
      "Results saved to job_results.txt\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":  #only runs when executed as a script\n",
    "    # Define keywords\n",
    "    keywords = role_keywords\n",
    "\n",
    "    #examples:\n",
    "    #keywords = [\"python\", \"django\", \"machine learning\", \"remote\"]\n",
    "    #keywords = role_keywords + tech_keywords + remote_keywords + salary_keywords\n",
    "    \n",
    "    # Define list of niche job board URLs to crawl, use the variable name \"urls\"\n",
    "    urls = top_scores_list\n",
    "    \n",
    "    # Create crawler and start crawling\n",
    "    crawler = JobCrawler(keywords=keywords, max_pages=10)\n",
    "    crawler.crawl(urls)\n",
    "    \n",
    "    # Optionally save results\n",
    "    crawler.save_results(\"job_results.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
